#!/usr/bin/python3

from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
import matplotlib.pyplot as plt
import subprocess as subp
import os, time, sys
import numpy as np
from sklearn.neural_network import MLPClassifier
from sklearn.externals import joblib
from termcolor import colored

# Returns the .wav files in current directory.
# Return value is a list of filenames.
def getWavFiles():
    files = subp.check_output(["ls"]).decode("utf8").split("\n")
    files = [f for f in files if f.split(".")[-1] == "wav"]
    return files

# Gets feature vector for a wav song
def getFeatureVector(file):
    try:
        [Fs, x] = audioBasicIO.readAudioFile(file);
        x = audioBasicIO.stereo2mono(x)
    except:
        print("Error on file: ", file)
        return None

    # We cut the audio to a 30 second window in the middle of the song
    # If the audio is shorter than 30 second, we discard it
    nSamples = int(Fs * 29.9)
    if isinstance(x, int):
        return None
    elif len(x) < nSamples:
        return None
    offset   = (len(x) - nSamples) // 2
    x = x[offset:offset+nSamples]

    mterm, sterm, f_names = audioFeatureExtraction.mtFeatureExtraction(x, Fs, 1*Fs, 1*Fs, 0.050*Fs, 0.025*Fs);

    # Should return 68 mid-term features per mid-term window (30 windows)
    return mterm.ravel()

# Returns the genres for each negative example.
# Return value is a dictionary of music_id -> genre
def getGenres():
    with open("genres.csv") as fp:
        genres = fp.read()

    gdict = {}
    genres = [ g.split(",") for g in genres.strip().split("\n") ]
    for row in genres:
        gdict[int(row[0])] = row[1]

    return gdict

# Returns the dataset composed of feature vectors and the class of each music
# Return value is a list containing:
#     1) a matrix where each row is [feature vector, class, music id, genre]
#     2) dictionary with counters of how many items per genre
def getDataset():
    dataset = []
    progress = 0
    genreCounter = {}

    # Get genre dictionary and initialize genre counters
    genres = getGenres()
    for g in set(genres.values()):
        genreCounter[g] = 0
    genreCounter["Classical"] = 0

    os.chdir("music_negative/")
    files = getWavFiles()
    for f in files:
        idd = int(f.rstrip(".wav"))   # Get music id from filename
        genre = genres[idd]           # Get music genre from dictionary
        if genreCounter[genre] > 400: # Don't get features for more than 400 songs per genre
            continue

        if progress % 10 == 0:
            print("Processing: {}".format(progress))

        features = getFeatureVector(f)
        if features is not None:
            dataset.append([features, 0, idd, genre])
            genreCounter[genre] += 1    # Increment genrecounter

        progress += 1

    os.chdir("..")

    os.chdir("music_positive/")
    files = getWavFiles()
    for f in files:
        if progress % 10 == 0:
            print("Processing: {}".format(progress))

        features = getFeatureVector(f)
        if features is not None:
            dataset.append([features, 1, -1, "Classical"])
            genreCounter["Classical"] += 1

        progress += 1

    os.chdir("..")

    return [np.array(dataset), genreCounter]

# Returns [dataset, filenames] for the songs to predict
def getPredictSongs():
    dataset = []

    os.chdir("predict/")
    files = getWavFiles()
    progress = 1
    total    = len(files)
    for f in files:
        print("Extracting features... {}/{}".format(progress, total))
        features = getFeatureVector(f)
        if features is not None:
            dataset.append([features, f])
        progress += 1

    os.chdir("..")
    return np.array(dataset)

# Trains an MLP using the given dataset.
# Saves the model if desired.
def trainModel(trainData, solve="lbfgs", hidden=(4), save=False):
    clf = MLPClassifier(solver=solve, alpha=1e-4, hidden_layer_sizes=hidden, max_iter=1000, activation="logistic")

    X = [ i[0] for i in trainData ]
    Y = [ i[1] for i in trainData ]
    clf.fit(X, Y)

    if save:
        joblib.dump(clf, 'mlp.model')

    return clf

# Returns saved model if it exists
def loadModel():
    try:
        return joblib.load('mlp.model')
    except FileNotFoundError:
        return None

def experimentMany(dataset):
    # Create a randomized copy of the dataset
    rDataset = dataset.copy()
    np.random.shuffle(rDataset)

    # Create statistics
    genres = set([ row[3] for row in dataset ])

    # k-fold cross-validation
    nFolds = 5
    foldSize = len(rDataset)//nFolds

    for solver in ["lbfgs", "adam", "sgd"]:
        print("{}\t{}\t{}\t{}\t{}".format("solver", "layrs", "acc", "train", "test"))

        for layers in [(1), (2), (3), (4), (5), (6), (7), (8)]:
            success = 0
            attempts = 0

            execTries = 10
            testTime  = 0
            trainTime = 0

            # Create statistics
            stats  = {}
            for g in genres:
                stats[g] = [0,0] # 0 Successes, 0 Failures

            for execId in range(execTries):
                for i in range(nFolds):
                    train_idx = np.array([ True for i in range(len(rDataset)) ])
                    train_idx[i*foldSize:(i+1)*foldSize] = False

                    if i == nFolds-1:
                        train_idx[-1] = False

                    test_idx = train_idx == False

                    train = rDataset[train_idx]
                    test  = rDataset[test_idx]

                    # train model
                    beg = time.time()
                    clf = trainModel(train, solver, layers)
                    trainTime += time.time() - beg

                    beg = time.time()

                    # Calculate accuracy
                    for [x,y,idd,genre] in test:
                        y_star = clf.predict([x])
                        # print("[{},{}]".format(y,y_star))
                        if y_star == y:
                            success += 1
                            stats[genre][0] += 1
                        else:
                            stats[genre][1] += 1
                        attempts += 1

                    testTime += time.time() - beg

            print("{}\t{}\t{}\t{}\t{}\t{}".format(solver, layers, success / attempts, trainTime / execTries, testTime / execTries, stats))

# Balances the dataset by replicating classical songs
def fillDataset(dataset):
    nOther = len(dataset[dataset[:,3] != "Classical"])
    nClass = len(dataset[dataset[:,3] == "Classical"])
    remaining = nOther - nClass

    if remaining < 0:
        return dataset

    nCopies = (remaining + nClass - 1) // nClass
    classical = dataset[dataset[:,3] == "Classical"]
    classical = np.concatenate([classical for i in range(nCopies)])

    return np.concatenate([dataset, classical])

def experimentOne(dataset):
    # Create a randomized copy of the dataset
    rDataset = dataset.copy()
    np.random.shuffle(rDataset)

    # Create statistics
    genres = set([ row[3] for row in dataset ])

    # k-fold cross-validation
    nFolds = 5
    foldSize = len(rDataset)//nFolds

    print("{}\t{}\t{}".format("acc", "train", "test"))

    success = 0
    attempts = 0

    execTries = 10
    testTime  = 0
    trainTime = 0

    # Create statistics
    stats  = {}
    for g in genres:
        stats[g] = [0,0] # 0 Successes, 0 Failures

    for execId in range(execTries):
        for i in range(nFolds):
            train_idx = np.array([ True for i in range(len(rDataset)) ])
            train_idx[i*foldSize:(i+1)*foldSize] = False

            if i == nFolds-1:
                train_idx[-1] = False

            test_idx = train_idx == False

            train = fillDataset(rDataset[train_idx])
            test  = rDataset[test_idx]

            # train model
            beg = time.time()
            clf = trainModel(train, "lbfgs", (4))
            trainTime += time.time() - beg

            beg = time.time()

            # Calculate accuracy
            for [x,y,idd,genre] in test:
                y_star = clf.predict([x])
                # print("[{},{}]".format(y,y_star))
                if y_star == y:
                    success += 1
                    stats[genre][0] += 1
                else:
                    stats[genre][1] += 1
                attempts += 1

            testTime += time.time() - beg

        print("{}\t{}\t{}\t{}".format(success / attempts, trainTime / execTries, testTime / execTries, stats))

# Normalizes the dataset so that each feature has mean 0 and deviation 1
def zNormalize(dataset):
    # Each row in the dataset has the feature vector, then its class
    feats = dataset[:,0]

    # Transform to matrix
    matrix = np.matrix([[col for col in row] for row in feats])

    # For each feature (column), we z-normalize it
    for col in range(matrix.shape[1]):
        mean = np.mean(matrix[:,col])
        std  = np.std(matrix[:,col])
        matrix[:,col] = (matrix[:,col] - mean) / std

    # Replace features in the dataset
    for row in range(matrix.shape[0]):
        dataset[row,0] = np.array(matrix[row,:]).flatten()

if len(sys.argv) == 1:
    helpmsg = """Usage: {} [option [option...]]

OPTIONS
=======
    -f    - generate features
    -e    - run experiments using K-fold cross validation
    -o    - run experiments on the selected model
    -c    - create neural network model
    -p    - predict classes of files in 'predict/' folder
""".format(sys.argv[0])

    print(helpmsg, end="")
    sys.exit(1)

# Get or generate dataset
if sys.argv.count("-f") > 0:
    [dataset, genreCounter] = getDataset()
    zNormalize(dataset)
    np.save("features.npy", dataset)
    for key in genreCounter:
        print("{}: {}".format(key, genreCounter[key]))
else:
    try:
        dataset = np.load("features.npy")
    except:
        print("Please generate features first. Run the program without arguments for help.")
        sys.exit(2)

# Execute experiments for multiple configurations
if sys.argv.count("-e") > 0:
    experimentMany(dataset)

# Execute experiments for the chosen configuration
if sys.argv.count("-o") > 0:
    experimentOne(dataset)

# Create model with the whole dataset
if sys.argv.count("-c") > 0:
    trainModel(dataset, "lbfgs", (4), save=True)

# Predict classes
if sys.argv.count("-p") > 0:
    clf = loadModel()
    songs = getPredictSongs()
    zNormalize(songs)

    fileColor = lambda x: colored(x, "cyan")
    classColor = lambda x: colored(x, "green", attrs=["bold"])
    nClassColor = lambda x: colored(x, "red", attrs=["bold"])

    if not clf:
        print("Please create the model using '-c' first.")
        sys.exit(3)

    # Get largest filename size
    maxSize = max([ len(i) for i in songs[:,1] ])
    maxSize = min(60, maxSize)

    for s in songs:
        y = clf.predict([s[0]])
        fileStr = "\"" + s[1] + "\""
        fileStr = fileStr.rjust(maxSize)[:maxSize]
        print("{} is {}".format(
                fileColor(fileStr),
                classColor("Classical") if y == 1 else nClassColor("Not Classical")
        ))
