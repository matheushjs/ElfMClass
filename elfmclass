#!/usr/bin/python3

from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
import matplotlib.pyplot as plt
import subprocess as subp
import os, time, sys
import numpy as np
from sklearn.neural_network import MLPClassifier
from sklearn.externals import joblib

# Returns the .wav files in current directory.
# Return value is a list of filenames.
def getWavFiles():
    files = subp.check_output(["ls"]).decode("utf8").split("\n")
    files = [f for f in files if f.split(".")[-1] == "wav"]
    return files

# Gets feature vector for a wav song
def getFeatureVector(file):
    try:
        [Fs, x] = audioBasicIO.readAudioFile(file);
        x = audioBasicIO.stereo2mono(x)
    except:
        print("Error on file: ", file)
        return None

    # We cut the audio to a 30 second window in the middle of the song
    # If the audio is shorter than 30 second, we discard it
    nSamples = Fs * 30
    if isinstance(x, int):
        return None
    elif len(x) < nSamples:
        return None
    offset   = (len(x) - nSamples) // 2
    x = x[offset:offset+nSamples]

    mterm, sterm, f_names = audioFeatureExtraction.mtFeatureExtraction(x, Fs, 1*Fs, 1*Fs, 0.050*Fs, 0.025*Fs);

    # Should return 68 mid-term features per mid-term window (30 windows)
    return mterm.ravel()

# Returns the genres for each negative example.
# Return value is a dictionary of music_id -> genre
def getGenres():
    with open("genres.csv") as fp:
        genres = fp.read()

    gdict = {}
    genres = [ g.split(",") for g in genres.strip().split("\n") ]
    for row in genres:
        gdict[int(row[0])] = row[1]

    return gdict

# Returns the dataset composed of feature vectors and the class of each music
# Return value is a matrix where each row is [feature vector, class]
def getDataset():
    dataset = []

    os.chdir("music_negative/")
    files = getWavFiles()
    for f in files:
        features = getFeatureVector(f)
        if features is not None:
            dataset.append([features, 0])
    os.chdir("..")

    os.chdir("music_positive/")
    files = getWavFiles()
    for f in files:
        features = getFeatureVector(f)
        if features is not None:
            dataset.append([features, 1])
    os.chdir("..")

    return np.array(dataset)

# Trains an MLP using the given dataset.
def trainModel(trainData, solve="lbfgs", hidden=(10)):
    clf = MLPClassifier(solver=solve, alpha=1e-4, hidden_layer_sizes=hidden, max_iter=1000, activation="logistic")

    X = [ i[0] for i in trainData ]
    Y = [ i[1] for i in trainData ]
    clf.fit(X, Y)

    return clf

def experiment(dataset):
    # Create a randomized copy of the dataset
    rDataset = dataset.copy()
    np.random.shuffle(rDataset)

    # k-fold cross-validation
    nFolds = 5
    foldSize = len(rDataset)//nFolds

    for solver in ["lbfgs", "adam", "sgd"]:
        print("{}\t{}\t{}\t{}\t{}".format("solver", "layrs", "acc", "train", "test"))

        for layers in [(1), (2), (3), (4), (5), (6), (7), (8), (9), (10), (20), (30), (40), (50), (60), (5,2), (5,4), (10,2), (10,4), (20,2), (20,4)]:
            success = 0
            attempts = 0

            execTries = 10
            testTime  = 0
            trainTime = 0

            for execId in range(execTries):
                for i in range(nFolds):
                    train_idx = np.array([ True for i in range(len(rDataset)) ])
                    train_idx[i*foldSize:(i+1)*foldSize] = False

                    if i == nFolds-1:
                        train_idx[-1] = False

                    test_idx = train_idx == False

                    train = rDataset[train_idx]
                    test  = rDataset[test_idx]

                    # train model
                    beg = time.time()
                    clf = trainModel(train, solver, layers)
                    trainTime += time.time() - beg

                    beg = time.time()

                    # Calculate accuracy
                    for [x,y] in test:
                        y_star = clf.predict([x])
                        # print("[{},{}]".format(y,y_star))
                        if y_star == y:
                            success += 1
                        attempts += 1

                    testTime += time.time() - beg

            print("{}\t{}\t{}\t{}\t{}".format(solver, layers, success / attempts, trainTime / execTries, testTime / execTries))

# Normalizes the dataset so that each feature has mean 0 and deviation 1
def zNormalize(dataset):
    # Each row in the dataset has the feature vector, then its class
    feats = dataset[:,0]

    # Transform to matrix
    matrix = np.matrix([[col for col in row] for row in feats])

    # For each feature (column), we z-normalize it
    for col in range(matrix.shape[1]):
        mean = np.mean(matrix[:,col])
        std  = np.std(matrix[:,col])
        matrix[:,col] = (matrix[:,col] - mean) / std

    # Replace features in the dataset
    for row in range(matrix.shape[0]):
        dataset[row,0] = np.array(matrix[row,:]).flatten()


if len(sys.argv) == 1:
    helpmsg = """Usage: {} [option [option...]]

OPTIONS
=======
    -f    - generate features
    -e    - run experiments using K-fold cross validation
""".format(sys.argv[0])

    print(helpmsg, end="")
    sys.exit(1)

# Get or generate dataset
if sys.argv.count("-f") > 0:
    dataset = getDataset()
    zNormalize(dataset)
    np.save("features.npy", dataset)
else:
    try:
        dataset = np.load("features.npy")
    except:
        print("Please generate features first. Run the program without arguments for help.")
        sys.exit(2)

# Execute experiments
if sys.argv.count("-e") > 0:
    experiment(dataset)
