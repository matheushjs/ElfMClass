#!/usr/bin/python3

from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import audioFeatureExtraction
import matplotlib.pyplot as plt
import subprocess as subp
import os, time, sys
import numpy as np
from sklearn.neural_network import MLPClassifier
from sklearn.externals import joblib

# Returns the .wav files in current directory.
# Return value is a list of filenames.
def getWavFiles():
    files = subp.check_output(["ls"]).decode("utf8").split("\n")
    files = [f for f in files if f.split(".")[-1] == "wav"]
    return files

# Gets feature vector for a wav song
def getFeatureVector(file):
    try:
        [Fs, x] = audioBasicIO.readAudioFile(file);
        x = audioBasicIO.stereo2mono(x)
    except:
        print("Error on file: ", file)
        return None

    # We cut the audio to a 30 second window in the middle of the song
    # If the audio is shorter than 30 second, we discard it
    nSamples = int(Fs * 29.9)
    if isinstance(x, int):
        return None
    elif len(x) < nSamples:
        return None
    offset   = (len(x) - nSamples) // 2
    x = x[offset:offset+nSamples]

    mterm, sterm, f_names = audioFeatureExtraction.mtFeatureExtraction(x, Fs, 1*Fs, 1*Fs, 0.050*Fs, 0.025*Fs);

    # Should return 68 mid-term features per mid-term window (30 windows)
    return mterm.ravel()

# Returns the genres for each negative example.
# Return value is a dictionary of music_id -> genre
def getGenres():
    with open("genres.csv") as fp:
        genres = fp.read()

    gdict = {}
    genres = [ g.split(",") for g in genres.strip().split("\n") ]
    for row in genres:
        gdict[int(row[0])] = row[1]

    return gdict

# Returns the dataset composed of feature vectors and the class of each music
# Return value is a list containing:
#     1) a matrix where each row is [feature vector, class, music id, genre]
#     2) dictionary with counters of how many items per genre
def getDataset():
    dataset = []
    progress = 0
    genreCounter = {}

    # Get genre dictionary and initialize genre counters
    genres = getGenres()
    for g in set(genres.values()):
        genreCounter[g] = 0
    genreCounter["Classical"] = 0

    os.chdir("music_negative/")
    files = getWavFiles()
    for f in files:
        idd = int(f.rstrip(".wav"))   # Get music id from filename
        genre = genres[idd]           # Get music genre from dictionary
        if genreCounter[genre] > 400: # Don't get features for more than 400 songs per genre
            continue

        if progress % 10 == 0:
            print("Processing: {}".format(progress))

        features = getFeatureVector(f)
        if features is not None:
            dataset.append([features, 0, idd, genre])
            genreCounter[genre] += 1    # Increment genrecounter

        progress += 1

    os.chdir("..")

    os.chdir("music_positive/")
    files = getWavFiles()
    for f in files:
        if progress % 10 == 0:
            print("Processing: {}".format(progress))

        features = getFeatureVector(f)
        if features is not None:
            dataset.append([features, 1, -1, "Classical"])
            genreCounter["Classical"] += 1

        progress += 1

    os.chdir("..")

    return [np.array(dataset), genreCounter]

# Trains an MLP using the given dataset.
def trainModel(trainData, solve="lbfgs", hidden=(10)):
    clf = MLPClassifier(solver=solve, alpha=1e-4, hidden_layer_sizes=hidden, max_iter=1000, activation="logistic")

    X = [ i[0] for i in trainData ]
    Y = [ i[1] for i in trainData ]
    clf.fit(X, Y)

    return clf

def experiment(dataset):
    # Create a randomized copy of the dataset
    rDataset = dataset.copy()
    np.random.shuffle(rDataset)

    # Create statistics
    genres = set([ row[3] for row in dataset ])

    # k-fold cross-validation
    nFolds = 5
    foldSize = len(rDataset)//nFolds

    for solver in ["lbfgs", "adam", "sgd"]:
        print("{}\t{}\t{}\t{}\t{}".format("solver", "layrs", "acc", "train", "test"))

        for layers in [(1), (2), (3), (4), (5), (6), (7), (8)]:
            success = 0
            attempts = 0

            execTries = 10
            testTime  = 0
            trainTime = 0

            # Create statistics
            stats  = {}
            for g in genres:
                stats[g] = [0,0] # 0 Successes, 0 Failures

            for execId in range(execTries):
                for i in range(nFolds):
                    train_idx = np.array([ True for i in range(len(rDataset)) ])
                    train_idx[i*foldSize:(i+1)*foldSize] = False

                    if i == nFolds-1:
                        train_idx[-1] = False

                    test_idx = train_idx == False

                    train = rDataset[train_idx]
                    test  = rDataset[test_idx]

                    # train model
                    beg = time.time()
                    clf = trainModel(train, solver, layers)
                    trainTime += time.time() - beg

                    beg = time.time()

                    # Calculate accuracy
                    for [x,y,idd,genre] in test:
                        y_star = clf.predict([x])
                        # print("[{},{}]".format(y,y_star))
                        if y_star == y:
                            success += 1
                            stats[genre][0] += 1
                        else:
                            stats[genre][1] += 1
                        attempts += 1

                    testTime += time.time() - beg

            print("{}\t{}\t{}\t{}\t{}\t{}".format(solver, layers, success / attempts, trainTime / execTries, testTime / execTries, stats))

# Normalizes the dataset so that each feature has mean 0 and deviation 1
def zNormalize(dataset):
    # Each row in the dataset has the feature vector, then its class
    feats = dataset[:,0]

    # Transform to matrix
    matrix = np.matrix([[col for col in row] for row in feats])

    # For each feature (column), we z-normalize it
    for col in range(matrix.shape[1]):
        mean = np.mean(matrix[:,col])
        std  = np.std(matrix[:,col])
        matrix[:,col] = (matrix[:,col] - mean) / std

    # Replace features in the dataset
    for row in range(matrix.shape[0]):
        dataset[row,0] = np.array(matrix[row,:]).flatten()


if len(sys.argv) == 1:
    helpmsg = """Usage: {} [option [option...]]

OPTIONS
=======
    -f    - generate features
    -e    - run experiments using K-fold cross validation
""".format(sys.argv[0])

    print(helpmsg, end="")
    sys.exit(1)

# Get or generate dataset
if sys.argv.count("-f") > 0:
    [dataset, genreCounter] = getDataset()
    zNormalize(dataset)
    np.save("features.npy", dataset)
    for key in genreCounter:
        print("{}: {}".format(key, genreCounter[key]))
else:
    try:
        dataset = np.load("features.npy")
    except:
        print("Please generate features first. Run the program without arguments for help.")
        sys.exit(2)

# Execute experiments
if sys.argv.count("-e") > 0:
    experiment(dataset)
